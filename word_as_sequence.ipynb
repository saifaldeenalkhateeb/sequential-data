{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\r\n",
        "  <td>\r\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/saifaldeenalkhateeb/sequential-data/blob/master/word_as_sequence.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\r\n",
        "  </td>\r\n",
        "  <td>\r\n",
        "    <a target=\"_blank\" href=\"https://github.com/saifaldeenalkhateeb/sequential-data/blob/master/word_as_sequence.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\r\n",
        "  </td>\r\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c6XWFun5o5T"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\r\n",
        "import os\r\n",
        "import pathlib\r\n",
        "import shutil\r\n",
        "import random\r\n",
        "from os import sys\r\n",
        "from termcolor import colored\r\n",
        "\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import numpy\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "from json import JSONEncoder, JSONDecoder\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CODE_DIR = \"/content/sequential-data/\"\r\n",
        "COLOAB_DIR = \"/content/\"\r\n",
        "SAVED_MODELS_DIR = CODE_DIR + \"saved-models/\"\r\n",
        "REPO = \"https://github.com/saifaldeenalkhateeb/sequential-data/\"\r\n",
        "sys.path.append(\".\")\r\n",
        "sys.path.append(\"..\")\r\n",
        "\r\n",
        "\r\n",
        "if os.path.exists(COLOAB_DIR) and not os.path.exists(CODE_DIR):\r\n",
        "    !git clone {REPO}\r\n",
        "\r\n",
        "os.chdir(CODE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTLSRA0fJuo5"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"glove.6B.zip\"):\n",
        "  !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "if not os.path.exists(\"glove.6B.100d.txt\"):\n",
        "  !unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU2em0ru5v-q"
      },
      "outputs": [],
      "source": [
        "max_length = 600\r\n",
        "# input_dim\r\n",
        "max_tokens = 20000\r\n",
        "batch_size = 32\r\n",
        "TRAINIGN_FALG = False\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgTyBt1G5xJ5"
      },
      "outputs": [],
      "source": [
        "class Text_Vectorization(Enum):\r\n",
        "    def unigram_int():\r\n",
        "        return TextVectorization(\r\n",
        "            max_tokens=max_tokens,\r\n",
        "            output_mode=\"int\",\r\n",
        "            output_sequence_length=max_length,\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "DATA = {\r\n",
        "    'unigram_int': Text_Vectorization.unigram_int()\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "class Setting:\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 simple_model_name: str,\r\n",
        "                 text_vectorization: str,\r\n",
        "                 units: int = 32,\r\n",
        "                 one_hot: bool = False,\r\n",
        "                 is_masked: bool = False,\r\n",
        "                 is_using_glove: bool = False,\r\n",
        "                 gru: bool = False,\r\n",
        "                 lstm: bool = False,\r\n",
        "                 simplernn: bool = False,\r\n",
        "                 input_dim=20000,\r\n",
        "                 output_dim=256,\r\n",
        "                 path_to_glove_file=\"glove.6B.100d.txt\"\r\n",
        "                 ):\r\n",
        "        self.simple_model_name = simple_model_name\r\n",
        "        self.is_masked = is_masked\r\n",
        "        self.is_using_glove = is_using_glove\r\n",
        "        self.gru = gru\r\n",
        "        self.lstm = lstm\r\n",
        "        self.simplernn = simplernn\r\n",
        "        self.units = units\r\n",
        "        self.one_hot = one_hot\r\n",
        "        self.input_dim = input_dim\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.text_vectorization = text_vectorization\r\n",
        "        self.path_to_glove_file = path_to_glove_file\r\n",
        "\r\n",
        "    def text_vectorization_func(self):\r\n",
        "        return DATA[self.text_vectorization]\r\n",
        "\r\n",
        "    def name(self):\r\n",
        "        model_name = \"\"\r\n",
        "        if self.text_vectorization is not None:\r\n",
        "            model_name += f\"{self.text_vectorization}_\"\r\n",
        "        else:\r\n",
        "            raise ValueError(\"text_vectorization_name musst be specified...\")\r\n",
        "\r\n",
        "        if self.one_hot:\r\n",
        "            model_name += \"one_hot_encoded_\"\r\n",
        "        if self.gru:\r\n",
        "            model_name += \"gru_\"\r\n",
        "        if self.simplernn:\r\n",
        "            model_name += \"simplernn_\"\r\n",
        "        if self.lstm:\r\n",
        "            model_name += \"lstm_\"\r\n",
        "        if self.is_masked:\r\n",
        "            model_name += \"masked_\"\r\n",
        "        if self.is_using_glove:\r\n",
        "            model_name += \"using_glove_\"\r\n",
        "\r\n",
        "        model_name += f\"input_dim_{self.input_dim}_\"\r\n",
        "        model_name += f\"output_dim_{self.output_dim}_\"\r\n",
        "        model_name += \"model\"\r\n",
        "\r\n",
        "        return model_name\r\n",
        "\r\n",
        "\r\n",
        "class SettingEncoder(JSONEncoder):\r\n",
        "    def default(self, o):\r\n",
        "        return o.__dict__\r\n",
        "\r\n",
        "\r\n",
        "def decode_setting(dct):\r\n",
        "    return Setting(\r\n",
        "        simple_model_name=dct[\"simple_model_name\"],\r\n",
        "        text_vectorization=dct[\"text_vectorization\"],\r\n",
        "        units=dct[\"units\"],\r\n",
        "        one_hot=dct[\"one_hot\"],\r\n",
        "        is_masked=dct[\"is_masked\"],\r\n",
        "        is_using_glove=dct[\"is_using_glove\"],\r\n",
        "        gru=dct[\"gru\"],\r\n",
        "        lstm=dct[\"lstm\"],\r\n",
        "        simplernn=dct[\"simplernn\"],\r\n",
        "        input_dim=dct[\"input_dim\"],\r\n",
        "        output_dim=dct[\"output_dim\"],\r\n",
        "        path_to_glove_file=dct[\"path_to_glove_file\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0iQimXU5yrh"
      },
      "outputs": [],
      "source": [
        "class IMDB_REVIEWING_ANALYSIS_WORD_AS_SEQUENCE:\r\n",
        "\r\n",
        "    def __init__(self, setting: Setting):\r\n",
        "        self.setting = setting\r\n",
        "        self.load_datasets()\r\n",
        "        self.build_validation_directory_and_fill_it()\r\n",
        "        self.text_vectorization = self.setting.text_vectorization_func()\r\n",
        "        self.prepaire_datasets()\r\n",
        "        if TRAINIGN_FALG:\r\n",
        "            self.model = self.build_model(setting)\r\n",
        "\r\n",
        "    def load_datasets(self):\r\n",
        "        if not os.path.exists('aclImdb'):\r\n",
        "            !curl - O https: // ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n",
        "            !tar - xf aclImdb_v1.tar.gz\r\n",
        "            !rm - r aclImdb/train/unsup\r\n",
        "\r\n",
        "    def build_validation_directory_and_fill_it(self):\r\n",
        "        base_dir = pathlib.Path(\"aclImdb\")\r\n",
        "        val_dir = base_dir / \"val\"\r\n",
        "        train_dir = base_dir / \"train\"\r\n",
        "        if not os.path.exists(val_dir):\r\n",
        "\r\n",
        "          for category in (\"neg\", \"pos\"):\r\n",
        "\r\n",
        "              path = val_dir / category\r\n",
        "              if not os.path.exists(path):\r\n",
        "                  os.makedirs(path)\r\n",
        "\r\n",
        "              files = os.listdir(train_dir / category)\r\n",
        "              random.Random(1337).shuffle(files)\r\n",
        "              num_val_samples = int(0.2 * len(files))\r\n",
        "              val_files = files[-num_val_samples:]\r\n",
        "\r\n",
        "              for fname in val_files:\r\n",
        "                  #src = train_dir / category / fname\r\n",
        "                  #dst = val_dir / category / fname\r\n",
        "                  src = f'{CODE_DIR}/{train_dir}/{category}/{fname}'\r\n",
        "                  dst = f'{CODE_DIR}/{val_dir}/{category}/'\r\n",
        "                  %mv {src} {dst}\r\n",
        "\r\n",
        "    def prepaire_datasets(self):\r\n",
        "        train_ds = keras.preprocessing.text_dataset_from_directory(\r\n",
        "            \"aclImdb/train\", batch_size=batch_size\r\n",
        "        )\r\n",
        "        val_ds = keras.preprocessing.text_dataset_from_directory(\r\n",
        "            \"aclImdb/val\", batch_size=batch_size\r\n",
        "        )\r\n",
        "        test_ds = keras.preprocessing.text_dataset_from_directory(\r\n",
        "            \"aclImdb/test\", batch_size=batch_size\r\n",
        "        )\r\n",
        "\r\n",
        "        text_only_train_ds = train_ds.map(lambda x, y: x)\r\n",
        "        self.text_vectorization.adapt(text_only_train_ds)\r\n",
        "\r\n",
        "        self.prepaired_train_ds = train_ds.map(\r\n",
        "            lambda x, y: (self.text_vectorization(x), y))\r\n",
        "        self.prepaired_val_ds = val_ds.map(\r\n",
        "            lambda x, y: (self.text_vectorization(x), y))\r\n",
        "        self.prepaired_test_ds = test_ds.map(\r\n",
        "            lambda x, y: (self.text_vectorization(x), y))\r\n",
        "\r\n",
        "    def set_callbacks(self, callbacks):\r\n",
        "        self.callbacks = callbacks\r\n",
        "\r\n",
        "    def train_model(self, epochs):\r\n",
        "        self.history = self.model.fit(\r\n",
        "            self.prepaired_train_ds.cache(),\r\n",
        "            validation_data=self.prepaired_val_ds.cache(),\r\n",
        "            epochs=epochs,\r\n",
        "            callbacks=self.callbacks)\r\n",
        "\r\n",
        "    def plot_training_curve(self):\r\n",
        "      if not self.history is None:\r\n",
        "          plt.plot(self.history.history['accuracy'])\r\n",
        "          plt.plot(self.history.history['val_accuracy'])\r\n",
        "          plt.title(self.setting.simple_model_name)\r\n",
        "          plt.ylabel('accuracy')\r\n",
        "          plt.xlabel('epoch')\r\n",
        "          plt.legend(['train', 'val'], loc='upper left')\r\n",
        "          self.save_training_curve(self.setting.simple_model_name, plt)\r\n",
        "          plt.show()\r\n",
        "      else:\r\n",
        "        print(\"history is not initialized\")\r\n",
        "\r\n",
        "    def load_model(self, model_name):\r\n",
        "        path = SAVED_MODELS_DIR + model_name\r\n",
        "        print(path)\r\n",
        "        self.model = keras.models.load_model(path)\r\n",
        "\r\n",
        "    def save_training_curve(self, name, plt):\r\n",
        "        plt.savefig(name)\r\n",
        "\r\n",
        "    def get_test_accuracy(self):\r\n",
        "        print(\r\n",
        "            f\"Test acc: {self.model.evaluate(self.prepaired_test_ds)[1]:.3f}\")\r\n",
        "\r\n",
        "    def forward(self, text: str):\r\n",
        "        inputs = keras.Input(shape=(1,), dtype=\"string\")\r\n",
        "        processed_inputs = self.text_vectorization(inputs)\r\n",
        "        outputs = self.model(processed_inputs)\r\n",
        "        inference_model = keras.Model(inputs, outputs)\r\n",
        "        prepaired_text = tf.convert_to_tensor([[text], ])\r\n",
        "        return inference_model(prepaired_text)\r\n",
        "\r\n",
        "    def build_model(self, model_setting: Setting):\r\n",
        "\r\n",
        "        inputs = keras.Input(shape=(None,), dtype=\"int64\")\r\n",
        "\r\n",
        "        if model_setting.one_hot:\r\n",
        "            embedded = tf.one_hot(inputs, depth=model_setting.input_dim)\r\n",
        "        else:\r\n",
        "\r\n",
        "            if model_setting.is_masked and model_setting.is_using_glove:\r\n",
        "                embedding_matrix = self.build_embedding_matrix(\r\n",
        "                    self.setting.path_to_glove_file)\r\n",
        "\r\n",
        "                embedded = layers.Embedding(model_setting.input_dim, model_setting.output_dim,\r\n",
        "                                            keras.initializers.Constant(\r\n",
        "                                                embedding_matrix),\r\n",
        "                                            trainable=False, mask_zero=True)(inputs)\r\n",
        "\r\n",
        "            elif model_setting.is_masked and not model_setting.is_using_glove:\r\n",
        "\r\n",
        "                embedded = layers.Embedding(input_dim=model_setting.input_dim,\r\n",
        "                                            output_dim=model_setting.output_dim, mask_zero=True)(inputs)\r\n",
        "            else:\r\n",
        "                embedded = layers.Embedding(\r\n",
        "                    input_dim=model_setting.input_dim, output_dim=model_setting.output_dim)(inputs)\r\n",
        "\r\n",
        "        if model_setting.lstm:\r\n",
        "            x = layers.Bidirectional(\r\n",
        "                layers.LSTM(model_setting.units))(embedded)\r\n",
        "        elif model_setting.gru:\r\n",
        "            x = layers.Bidirectional(layers.GRU(model_setting.units))(embedded)\r\n",
        "        elif model_setting.simplernn:\r\n",
        "            x = layers.Bidirectional(\r\n",
        "                layers.SimpleRNN(model_setting.units))(embedded)\r\n",
        "        else:\r\n",
        "            x = layers.Bidirectional(\r\n",
        "                layers.LSTM(model_setting.units))(embedded)\r\n",
        "\r\n",
        "        x = layers.Dropout(0.5)(x)\r\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\r\n",
        "        model = keras.Model(inputs, outputs)\r\n",
        "        model.compile(optimizer=\"rmsprop\",\r\n",
        "                      loss=\"binary_crossentropy\",\r\n",
        "                      metrics=[\"accuracy\"])\r\n",
        "        return model\r\n",
        "\r\n",
        "    def build_embedding_matrix(self, path_to_glove_file):\r\n",
        "\r\n",
        "        embeddings_index = {}\r\n",
        "        with open(path_to_glove_file) as f:\r\n",
        "            for line in f:\r\n",
        "                word, coefs = line.split(maxsplit=1)\r\n",
        "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\r\n",
        "                embeddings_index[word] = coefs\r\n",
        "\r\n",
        "        vocabulary = self.text_vectorization.get_vocabulary()\r\n",
        "        word_index = dict(zip(vocabulary, range(len(vocabulary))))\r\n",
        "\r\n",
        "        embedding_matrix = np.zeros(\r\n",
        "            (self.setting.input_dim, self.setting.output_dim))\r\n",
        "        for word, i in word_index.items():\r\n",
        "            if i < max_tokens:\r\n",
        "                embedding_vector = embeddings_index.get(word)\r\n",
        "            if embedding_vector is not None:\r\n",
        "                embedding_matrix[i] = embedding_vector\r\n",
        "        return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ3zjpsp500h"
      },
      "outputs": [],
      "source": [
        "#with open('settings.json', 'w') as f:\r\n",
        "#    json.dump(settings, f, indent=4, cls=SettingEncoder)\r\n",
        "with open(\"settings.json\") as settings_data:\r\n",
        "    data = settings_data.read()\r\n",
        "    settings = json.loads(data, object_hook=decode_setting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDdb4u0w52DS",
        "outputId": "4fd06be9-7231-4b06-bbad-14524ab68093"
      },
      "outputs": [],
      "source": [
        "models = []\r\n",
        "\r\n",
        "if TRAINIGN_FALG:\r\n",
        "  for setting in settings:\r\n",
        "      callbacks = [keras.callbacks.ModelCheckpoint(\r\n",
        "          f\"{setting.name()}.keras\", save_best_only=True)]\r\n",
        "      model = IMDB_REVIEWING_ANALYSIS_WORD_AS_SEQUENCE(setting)\r\n",
        "      model.set_callbacks(callbacks)\r\n",
        "      models.append(model)\r\n",
        "else:\r\n",
        "  for setting in settings:\r\n",
        "    model = IMDB_REVIEWING_ANALYSIS_WORD_AS_SEQUENCE(setting)\r\n",
        "    model.load_model(setting.name()+\".h5\")\r\n",
        "    models.append(model)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "33lLQZ4H53fz",
        "outputId": "ead14e2f-abad-45df-d48c-b454206b3b4f"
      },
      "outputs": [],
      "source": [
        "if TRAINIGN_FALG:\r\n",
        "  for model in models:\r\n",
        "      model.train_model(20)\r\n",
        "  for model in models:\r\n",
        "      model.plot_training_curve()\r\n",
        "for model in models:\r\n",
        "    model.get_test_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v7R-U7embDr"
      },
      "outputs": [],
      "source": [
        "models_with_model_name = {}\r\n",
        "for model in models:\r\n",
        "    models_with_model_name[model.setting.simple_model_name] = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s69BFlZN56Vj"
      },
      "outputs": [],
      "source": [
        "positive_text = \"That was an excellent movie, I loved it.\"\n",
        "negative_text = \"That was a bad movie, I hated it.\"\n",
        "middle1_text = \"That was a bad movie, I loved it.\"\n",
        "middle2_text = \"That was an excellent movie, I hated it.\"\n",
        "minh_text = \"I hated it. That was a sad movie.\"\n",
        "\n",
        "sentences = [positive_text, negative_text, middle1_text, middle2_text, minh_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQuDYYoi57v0"
      },
      "outputs": [],
      "source": [
        "def convert_to_tensor(text):\n",
        "    return tf.convert_to_tensor([[text],])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyF6xypb584z"
      },
      "outputs": [],
      "source": [
        "def predict_models(models, text, is_in_tensor: bool):\n",
        "    print(colored(text, 'green'))\n",
        "    for items in models.items():\n",
        "        model_name = items[0]\n",
        "        model = items[1]\n",
        "        if not is_in_tensor:\n",
        "            predictions = model.forward(convert_to_tensor(text))\n",
        "        else: \n",
        "            predictions = model.forward(text)\n",
        "        \n",
        "        print(f\"{model_name}: \", colored(f\"{float(predictions[0] * 100):.2f}\", 'red') + \" percent positive\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whjI3Yy85-MS",
        "outputId": "31b4d9a5-e511-484b-b24c-83a1723161c0"
      },
      "outputs": [],
      "source": [
        "for sentence in sentences:\r\n",
        "    predict_models(models_with_model_name, sentence, True)\r\n",
        "    print(\"- - - - - - - -  - - - - - - - - - \")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "word_as_sequence.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "28ceb70653115348cae0180c324ea5c374eb5bc85fd0f431db28cf1ac983bd60"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}